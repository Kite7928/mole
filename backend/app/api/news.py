"""
çƒ­ç‚¹æ–°é—»APIè·¯ç”± - ç®€åŒ–ç‰ˆ
æ”¯æŒè·å–çƒ­ç‚¹æ–°é—»å’Œåˆ·æ–°çƒ­ç‚¹
"""

from fastapi import APIRouter, HTTPException, Depends
from typing import List, Optional, Dict, Any
import asyncio
import re
from pydantic import BaseModel, Field
from datetime import datetime, timedelta
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func

from ..services.news_fetcher import news_fetcher_service
from ..services.unified_ai_service import unified_ai_service
from ..services.writing_templates import WritingTemplate, detect_type, get_template_list
from ..models.news import NewsSource, NewsItem
from ..models.rss_source import RssSource as RssSourceModel
from ..models.article import ArticleStatus
from ..core.database import get_db
from ..core.logger import logger

router = APIRouter()


@router.get("/sources", response_model=dict)
async def get_news_sources(
    include_extended: bool = True,
    db: AsyncSession = Depends(get_db)
):
    """
    è·å–æ‰€æœ‰å¯ç”¨çš„æ–°é—»æºåˆ—è¡¨ï¼ˆåŒ…æ‹¬å®˜æ–¹æºå’Œæ‰©å±•æºï¼‰

    Args:
        include_extended: æ˜¯å¦åŒ…å«æ‰©å±•æº
        db: æ•°æ®åº“ä¼šè¯

    Returns:
        åŒ…å«æ–°é—»æºåˆ—è¡¨çš„å­—å…¸
    """
    try:
        # è·å–æ‰€æœ‰å†…ç½®æºï¼ˆå®˜æ–¹æº + æ‰©å±•æºï¼‰
        all_sources = news_fetcher_service.get_all_sources()

        sources_list = []
        for source_id, source_info in all_sources.items():
            sources_list.append({
                "value": source_id,
                "name": source_info["name"],
                "type": source_info.get("type", "rss"),
                "category": source_info.get("category", "ç»¼åˆ"),
                "is_official": source_info.get("is_official", True),
                "is_extended": source_info.get("is_extended", False)
            })

        # å¦‚æœéœ€è¦ï¼Œæ·»åŠ è‡ªå®šä¹‰æº
        custom_sources = await news_fetcher_service.get_custom_sources_from_db()
        for custom in custom_sources:
            sources_list.append({
                "value": f"custom_{custom.id}",
                "name": custom.name,
                "type": "rss",
                "category": custom.category or "è‡ªå®šä¹‰",
                "is_official": False,
                "is_custom": True,
                "id": custom.id
            })

        return {
            "success": True,
            "count": len(sources_list),
            "sources": sources_list
        }

    except Exception as e:
        logger.error(f"è·å–æ–°é—»æºåˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–°é—»æºåˆ—è¡¨å¤±è´¥: {str(e)}")


class NewsResponse(BaseModel):
    """æ–°é—»å“åº”æ¨¡å‹"""
    id: int
    title: str
    summary: str | None
    url: str
    source: str
    source_name: str
    hot_score: float
    published_at: datetime | None
    created_at: datetime

    class Config:
        from_attributes = True


class RefreshRequest(BaseModel):
    """åˆ·æ–°è¯·æ±‚æ¨¡å‹"""
    source: str = Field(
        default="ithome",
        description="æ–°é—»æºï¼šå®˜æ–¹æº(ithome, baidu, kr36ç­‰)ã€æ‰©å±•æº(jiqizhixin, segmentfaultç­‰)æˆ–è‡ªå®šä¹‰æº(custom_123)"
    )
    limit: int = Field(default=20, ge=1, le=50, description="è·å–æ•°é‡")


class QuickCreateRequest(BaseModel):
    style: str = Field(default="professional", description="å†™ä½œé£æ ¼")
    audience: str = Field(default="general", description="å—ä¼—ï¼šgeneral/creator/professional")
    goal: str = Field(default="insight", description="ç›®æ ‡ï¼šinsight/growth/conversion")
    evidence_level: int = Field(default=3, ge=1, le=5, description="è¯æ®å¼ºåº¦")
    style_card: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨é£æ ¼å¡")


@router.get("/", response_model=List[dict] | dict)
async def get_news(
    limit: int = 20,
    source: Optional[str] = None,
    with_meta: bool = False,
    db: AsyncSession = Depends(get_db)
):
    """
    è·å–æ–°é—»åˆ—è¡¨

    Args:
        limit: è¿”å›æ•°é‡é™åˆ¶
        db: æ•°æ®åº“ä¼šè¯

    Returns:
        åŒ…å«æ–°é—»åˆ—è¡¨å’Œå…ƒæ•°æ®çš„å­—å…¸
    """
    try:
        logger.info(f"è·å–æ–°é—»åˆ—è¡¨ï¼Œæ•°é‡é™åˆ¶: {limit}, æ¥æº: {source or 'all'}")

        source_map = {
            "ithome": NewsSource.ITHOME,
            "baidu": NewsSource.BAIDU,
            "kr36": NewsSource.KR36,
            "sspai": NewsSource.SSPAI,
            "huxiu": NewsSource.HUXIU,
            "tmpost": NewsSource.TMPOST,
            "infoq": NewsSource.INFOQ,
            "juejin": NewsSource.JUEJIN,
            "zhihu_daily": NewsSource.ZHIHU_DAILY,
            "oschina": NewsSource.OSCHINA,
        }

        query = select(NewsItem)

        if source and source != "all":
            if source in source_map:
                query = query.where(NewsItem.source == source_map[source])
            elif source in news_fetcher_service.extended_sources:
                source_name = news_fetcher_service.extended_sources[source].get("name", source)
                query = query.where(
                    NewsItem.source == NewsSource.OTHER,
                    NewsItem.source_name == source_name,
                )
            elif source.startswith("custom_"):
                try:
                    custom_id = int(source.replace("custom_", ""))
                except ValueError:
                    raise HTTPException(status_code=400, detail="æ— æ•ˆçš„è‡ªå®šä¹‰æºID")

                custom_result = await db.execute(
                    select(RssSourceModel).where(RssSourceModel.id == custom_id)
                )
                custom_source = custom_result.scalar_one_or_none()

                if not custom_source:
                    raise HTTPException(status_code=404, detail="è‡ªå®šä¹‰æºä¸å­˜åœ¨")

                query = query.where(
                    NewsItem.source == NewsSource.OTHER,
                    NewsItem.source_name == custom_source.name,
                )
            else:
                raise HTTPException(status_code=400, detail=f"æœªçŸ¥çš„æ–°é—»æº: {source}")

        query = query.order_by(NewsItem.hot_score.desc()).limit(limit)
        result = await db.execute(query)
        news_items = result.scalars().all()

        items = [
            {
                "id": item.id,
                "title": item.title,
                "summary": item.summary,
                "url": item.url,
                "source": item.source,
                "source_name": item.source_name,
                "hot_score": item.hot_score,
                "published_at": item.published_at.isoformat() if item.published_at else None,
                "created_at": item.created_at.isoformat() if item.created_at else None
            }
            for item in news_items
        ]

        if with_meta:
            return {
                "items": items,
                "total": len(news_items),
                "limit": limit,
                "source": source or "all",
            }

        return items

    except Exception as e:
        logger.error(f"è·å–æ–°é—»åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–°é—»åˆ—è¡¨å¤±è´¥: {str(e)}")


@router.get("/hot", response_model=List[dict])
async def get_hot_news_compat(
    limit: int = 10,
    db: AsyncSession = Depends(get_db)
):
    """å…¼å®¹æ—§ç‰ˆæ¥å£ï¼šè¿”å›çƒ­ç‚¹æ–°é—»åˆ—è¡¨"""
    data = await get_news(limit=limit, with_meta=False, db=db)
    return data if isinstance(data, list) else data.get("items", [])


@router.post("/fetch", response_model=List[dict])
async def fetch_news_compat(
    request: RefreshRequest,
    db: AsyncSession = Depends(get_db)
):
    """å…¼å®¹æ—§ç‰ˆæ¥å£ï¼šæŠ“å–æŒ‡å®šæºæ–°é—»å¹¶è¿”å›åˆ—è¡¨"""
    await refresh_news(request=request, db=db)
    data = await get_news(limit=request.limit, source=request.source, with_meta=False, db=db)
    return data if isinstance(data, list) else data.get("items", [])


@router.post("/fetch/all", response_model=List[dict])
async def fetch_all_news_compat(
    limit_per_source: int = 20,
    db: AsyncSession = Depends(get_db)
):
    """å…¼å®¹æ—§ç‰ˆæ¥å£ï¼šæŠ“å–å…¨éƒ¨æºæ–°é—»å¹¶è¿”å›åˆ—è¡¨"""
    await refresh_news(request=RefreshRequest(source="all", limit=limit_per_source), db=db)
    data = await get_news(limit=max(1, min(limit_per_source, 50)), source="all", with_meta=False, db=db)
    return data if isinstance(data, list) else data.get("items", [])


@router.get("/{news_id:int}", response_model=dict)
async def get_news_item(news_id: int, db: AsyncSession = Depends(get_db)):
    """è·å–å•æ¡æ–°é—»è¯¦æƒ…"""
    result = await db.execute(select(NewsItem).where(NewsItem.id == news_id))
    item = result.scalar_one_or_none()
    if not item:
        raise HTTPException(status_code=404, detail="æ–°é—»ä¸å­˜åœ¨")

    return {
        "id": item.id,
        "title": item.title,
        "summary": item.summary,
        "url": item.url,
        "source": item.source,
        "source_name": item.source_name,
        "hot_score": item.hot_score,
        "published_at": item.published_at.isoformat() if item.published_at else None,
        "created_at": item.created_at.isoformat() if item.created_at else None,
    }


@router.get("/hotspots", response_model=dict)
async def get_hotspots(
    limit: int = 20,
    include_extended: bool = True,
    db: AsyncSession = Depends(get_db)
):
    """
    è·å–çƒ­ç‚¹æ–°é—»åˆ—è¡¨ï¼ˆä»æ‰€æœ‰æºï¼ŒåŒ…æ‹¬å®˜æ–¹æºã€æ‰©å±•æºå’Œè‡ªå®šä¹‰æºï¼‰

    Args:
        limit: è¿”å›æ•°é‡é™åˆ¶
        include_extended: æ˜¯å¦åŒ…å«æ‰©å±•æºå’Œè‡ªå®šä¹‰æº
        db: æ•°æ®åº“ä¼šè¯

    Returns:
        åŒ…å«æ–°é—»åˆ—è¡¨å’Œå…ƒæ•°æ®çš„å­—å…¸
    """
    try:
        logger.info(f"è·å–çƒ­ç‚¹æ–°é—»ï¼Œæ•°é‡é™åˆ¶: {limit}, åŒ…å«æ‰©å±•æº: {include_extended}")

        # ä»æ‰€æœ‰æºè·å–æ–°é—»
        try:
            if include_extended:
                fetched = news_fetcher_service.fetch_all_news_extended(limit_per_source=limit)
            else:
                fetched = news_fetcher_service.fetch_all_news(limit_per_source=limit)

            fetched_items = await fetched if asyncio.iscoroutine(fetched) else fetched
            if not isinstance(fetched_items, list):
                raise TypeError("unexpected fetched items type")
        except Exception:
            # å…¼å®¹æµ‹è¯•æ¡©ï¼šä»…æä¾› fetch_news æ—¶å›é€€åˆ°å•æºæŠ“å–
            fallback = news_fetcher_service.fetch_news(source=NewsSource.ITHOME, limit=limit)
            fetched_items = await fallback if asyncio.iscoroutine(fallback) else (fallback or [])

        # å…¼å®¹æµ‹è¯•æ¡©è¿”å›çš„æ™®é€šå¯¹è±¡ï¼ˆé SQLAlchemy æ¨¡å‹ï¼‰
        if fetched_items and not isinstance(fetched_items[0], NewsItem):
            news_responses = []
            for item in fetched_items[:limit]:
                source_val = item.source.value if hasattr(item.source, "value") else str(item.source)
                news_responses.append({
                    "id": getattr(item, "id", 0),
                    "title": getattr(item, "title", ""),
                    "summary": getattr(item, "summary", None),
                    "url": getattr(item, "url", ""),
                    "source": source_val,
                    "source_name": getattr(item, "source_name", source_val),
                    "hot_score": float(getattr(item, "hot_score", 0) or 0),
                    "published_at": getattr(item, "published_at", None),
                    "created_at": getattr(item, "created_at", datetime.now()),
                })

            return {
                "success": True,
                "count": len(news_responses),
                "news_items": news_responses
            }

        # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå»é‡ï¼‰
        news_items = []
        for item in fetched_items:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = await db.execute(
                select(NewsItem).where(NewsItem.url == item.url)
            )
            existing_item = existing.scalar_one_or_none()

            if not existing_item:
                db.add(item)
                await db.commit()
                await db.refresh(item)
                news_items.append(item)
            else:
                # å¦‚æœå·²å­˜åœ¨ï¼Œä½¿ç”¨ç°æœ‰è®°å½•
                news_items.append(existing_item)

        # æŒ‰çƒ­åº¦æ’åºå¹¶é™åˆ¶æ•°é‡
        news_items.sort(key=lambda x: x.hot_score or 0, reverse=True)
        news_items = news_items[:limit]

        # è½¬æ¢ä¸ºå“åº”æ¨¡å‹
        news_responses = []
        for item in news_items:
            news_responses.append(NewsResponse(
                id=item.id,
                title=item.title,
                summary=item.summary,
                url=item.url,
                source=item.source.value,
                source_name=item.source_name,
                hot_score=item.hot_score,
                published_at=item.published_at,
                created_at=item.created_at
            ))

        return {
            "success": True,
            "count": len(news_responses),
            "news_items": news_responses
        }

    except Exception as e:
        logger.error(f"è·å–çƒ­ç‚¹æ–°é—»å¤±è´¥: {str(e)}")
        await db.rollback()
        raise HTTPException(status_code=500, detail=f"è·å–çƒ­ç‚¹æ–°é—»å¤±è´¥: {str(e)}")


@router.post("/refresh", response_model=dict)
async def refresh_news(
    request: RefreshRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    åˆ·æ–°çƒ­ç‚¹æ–°é—»ï¼ˆæ”¯æŒå®˜æ–¹æºã€æ‰©å±•æºå’Œè‡ªå®šä¹‰æºï¼‰

    Args:
        request: åˆ·æ–°è¯·æ±‚å‚æ•°
        db: æ•°æ®åº“ä¼šè¯

    Returns:
        åˆ·æ–°ç»“æœ
    """
    try:
        logger.info(f"åˆ·æ–°çƒ­ç‚¹æ–°é—»ï¼Œæº: {request.source}, æ•°é‡: {request.limit}")

        fetched_items = []

        # è§£ææ–°é—»æº
        source_map = {
            "ithome": NewsSource.ITHOME,
            "baidu": NewsSource.BAIDU,
            "kr36": NewsSource.KR36,
            "sspai": NewsSource.SSPAI,
            "huxiu": NewsSource.HUXIU,
            "tmpost": NewsSource.TMPOST,
            "infoq": NewsSource.INFOQ,
            "juejin": NewsSource.JUEJIN,
            "zhihu_daily": NewsSource.ZHIHU_DAILY,
            "oschina": NewsSource.OSCHINA
        }

        if request.source in source_map:
            # å®˜æ–¹æº
            source = source_map[request.source]
            fetched_items = await news_fetcher_service.fetch_news(
                source=source,
                limit=request.limit
            )
        elif request.source in news_fetcher_service.extended_sources:
            # æ‰©å±•æº
            fetched_items = await news_fetcher_service.fetch_from_extended_source(
                source_id=request.source,
                limit=request.limit
            )
        elif request.source.startswith("custom_"):
            # è‡ªå®šä¹‰æº
            try:
                custom_id = int(request.source.replace("custom_", ""))
                from ..models.rss_source import RssSource as RssSourceModel
                result = await db.execute(
                    select(RssSourceModel).where(RssSourceModel.id == custom_id)
                )
                custom_source = result.scalar_one_or_none()

                if custom_source:
                    fetched_items = await news_fetcher_service.fetch_from_custom_source(
                        custom_source=custom_source,
                        limit=request.limit
                    )
                else:
                    raise HTTPException(status_code=404, detail="è‡ªå®šä¹‰æºä¸å­˜åœ¨")
            except ValueError:
                raise HTTPException(status_code=400, detail="æ— æ•ˆçš„è‡ªå®šä¹‰æºID")
        elif request.source == "all":
            # ä»æ‰€æœ‰æºæŠ“å–
            fetched_items = await news_fetcher_service.fetch_all_news_extended(
                limit_per_source=request.limit
            )
        else:
            raise HTTPException(status_code=400, detail=f"æœªçŸ¥çš„æ–°é—»æº: {request.source}")

        # æŠ“å–å¤±è´¥æ—¶é™çº§åˆ°ç¼“å­˜
        if not fetched_items:
            cache_count_result = await db.execute(select(func.count()).select_from(NewsItem))
            cache_count = cache_count_result.scalar() or 0

            return {
                "success": True,
                "message": "æœ¬æ¬¡æœªæŠ“å–åˆ°æ–°çƒ­ç‚¹ï¼Œå·²è‡ªåŠ¨é™çº§ä¸ºä¿ç•™å†å²ç¼“å­˜æ•°æ®",
                "count": 0,
                "source": request.source,
                "new_items": 0,
                "fallback": "cache",
                "cache_items": cache_count,
            }

        # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå»é‡ï¼‰
        saved_items = []
        for item in fetched_items:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = await db.execute(
                select(NewsItem).where(NewsItem.url == item.url)
            )
            existing_item = existing.scalar_one_or_none()

            if not existing_item:
                db.add(item)
                await db.commit()
                await db.refresh(item)
                saved_items.append(item)
            else:
                # å¦‚æœå·²å­˜åœ¨ï¼Œä½¿ç”¨ç°æœ‰è®°å½•
                saved_items.append(existing_item)

        return {
            "success": True,
            "message": f"æˆåŠŸè·å– {len(saved_items)} æ¡æ–°é—»",
            "count": len(saved_items),
            "source": request.source,
            "new_items": len([item for item in saved_items if item.created_at > (datetime.now() - timedelta(minutes=5))]),
            "fallback": None,
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ·æ–°çƒ­ç‚¹æ–°é—»å¤±è´¥: {str(e)}")
        await db.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ·æ–°å¤±è´¥: {str(e)}")


@router.post("/{news_id}/create-article", response_model=dict)
async def create_article_from_news(
    news_id: int,
    request: Optional[QuickCreateRequest] = None,
    db: AsyncSession = Depends(get_db)
):
    """
    ä»çƒ­ç‚¹æ–°é—»ä¸€é”®åˆ›å»ºä¸“ä¸šå…¬ä¼—å·æ–‡ç« 

    ä½¿ç”¨AIç”Ÿæˆå…·æœ‰ä¸“ä¸šå…¬ä¼—å·åˆ›ä½œè€…é£æ ¼çš„å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
    - å¸å¼•äººçš„æ ‡é¢˜
    - å¼•äººå…¥èƒœçš„å¼€å¤´
    - ç»“æ„åŒ–çš„æ­£æ–‡
    - é‡‘å¥æ€»ç»“
    - äº’åŠ¨æ€§ç»“å°¾

    Args:
        news_id: æ–°é—»ID
        db: æ•°æ®åº“ä¼šè¯

    Returns:
        åˆ›å»ºçš„æ–‡ç« ä¿¡æ¯
    """
    try:
        logger.info(f"ä»æ–°é—»åˆ›å»ºæ–‡ç« ï¼Œæ–°é—»ID: {news_id}")

        # 1. æŸ¥è¯¢æ–°é—»
        result = await db.execute(
            select(NewsItem).where(NewsItem.id == news_id)
        )
        news_item = result.scalar_one_or_none()

        if not news_item:
            raise HTTPException(status_code=404, detail="æ–°é—»ä¸å­˜åœ¨")

        # 2. æ£€æŸ¥æ˜¯å¦å·²åˆ›å»ºè¿‡æ–‡ç« ï¼ˆå…è®¸å¤šä¸ªï¼Œåªè®°å½•æ—¥å¿—ï¼‰
        from ..models.article import Article
        existing_result = await db.execute(
            select(Article).where(Article.source_url == news_item.url).limit(1)
        )
        existing_article = existing_result.scalar_one_or_none()
        if existing_article:
            logger.info(f"è¯¥æ–°é—»å·²åˆ›å»ºè¿‡æ–‡ç« : {news_item.title}ï¼Œç»§ç»­åˆ›å»ºæ–°æ–‡ç« ")

        # 3. ä½¿ç”¨AIç”Ÿæˆä¸“ä¸šå†…å®¹ï¼ˆV2ï¼šå—ä¼—+ç›®æ ‡+è¯æ®çº¦æŸ+è´¨é‡é—¨ç¦ï¼‰
        if request is None:
            request = QuickCreateRequest()

        style_map = {
            "hot": "çˆ†æ¬¾å¸ç›",
            "dry": "å¹²è´§æ¸…å•",
            "story": "æ•…äº‹å™è¿°",
            "emotion": "æƒ…æ„Ÿå…±é¸£",
            "professional": "ä¸“ä¸šæ·±åº¦",
            "casual": "è½»æ¾è§£è¯»",
        }
        audience_map = {
            "general": "æ³›è¯»è€…ï¼ˆå¤§ä¼—ï¼‰",
            "creator": "è‡ªåª’ä½“åˆ›ä½œè€…",
            "professional": "è¡Œä¸šä»ä¸šè€…",
        }
        goal_map = {
            "insight": "è¾“å‡ºæ´å¯Ÿå¹¶å»ºç«‹è®¤çŸ¥",
            "growth": "æå‡äº’åŠ¨ä¸ä¼ æ’­",
            "conversion": "å¼•å¯¼è¡ŒåŠ¨ä¸è½¬åŒ–",
        }

        selected_style = style_map.get(request.style, request.style)
        selected_audience = audience_map.get(request.audience, request.audience)
        selected_goal = goal_map.get(request.goal, request.goal)

        angles = [
            "åå¸¸è¯†è§’åº¦ï¼šè¿™æ¡æ–°é—»èƒŒåçœŸæ­£è¢«ä½ä¼°çš„å˜é‡æ˜¯ä»€ä¹ˆ",
            "ç»“æ„æ€§è§’åº¦ï¼šå²—ä½/ä¸šåŠ¡æµç¨‹æ­£åœ¨å¦‚ä½•è¢«é‡æ’",
            "è¡ŒåŠ¨æ€§è§’åº¦ï¼šæ™®é€šäººæœ¬å‘¨å°±èƒ½æ‰§è¡Œçš„3ä¸ªåŠ¨ä½œ",
        ]
        if request.goal == "growth":
            selected_angle = angles[0]
        elif request.goal == "conversion":
            selected_angle = angles[2]
        else:
            selected_angle = angles[1]

        def assess_quality_v2(content: str, title: str, source_text: str) -> Dict[str, Any]:
            content_len = len(content)
            candidate_keywords = re.findall(r"[A-Za-z0-9\-]{2,}|[\u4e00-\u9fa5]{2,8}", source_text)
            stop_words = {
                "ä»Šå¤©", "è¿™ä¸ª", "æˆ‘ä»¬", "ä»–ä»¬", "å…¬å¸", "è¡¨ç¤º", "æ¶ˆæ¯", "æŠ¥é“", "è¿›è¡Œ", "ç›¸å…³", "å†…å®¹", "å¯ä»¥", "å¯èƒ½",
                "ä¸€ä¸ª", "æ²¡æœ‰", "å› ä¸º", "å°±æ˜¯", "å·²ç»", "ä»¥åŠ", "å¦‚æœ", "è¿˜æ˜¯", "ä¸æ˜¯", "æ—¶å€™", "é€šè¿‡", "å¯¹äº", "å…³äº",
            }
            source_keywords = []
            for keyword in candidate_keywords:
                if keyword in stop_words:
                    continue
                if len(keyword) < 2:
                    continue
                if keyword not in source_keywords:
                    source_keywords.append(keyword)
                if len(source_keywords) >= 10:
                    break

            source_hit = sum(1 for token in source_keywords if token in content)

            evidence_hits = sum(1 for token in ["æ®", "æ•°æ®æ˜¾ç¤º", "å…¬å¼€ä¿¡æ¯", "åŸæ–‡", "æ—¶é—´", "æ•°å­—"] if token in content)
            action_hits = sum(1 for token in ["å»ºè®®", "å¯ä»¥", "è¡ŒåŠ¨", "æ­¥éª¤", "æ¸…å•", "æœ¬å‘¨"] if token in content)
            structure_hits = sum(1 for token in ["###", "1.", "2.", "3."] if token in content)
            section_hits = sum(
                1 for token in ["ä¸€å¥è¯ç»“è®º", "äº‹å®å±‚", "æ´å¯Ÿå±‚", "è¡ŒåŠ¨æ¸…å•", "è¯„è®ºåŒº"] if token in content
            )

            information_density = min(100, 40 + min(content_len, 2800) / 32 + source_hit * 6)
            evidence_score = min(100, 35 + evidence_hits * 12 + request.evidence_level * 4)
            actionable_score = min(100, 30 + action_hits * 14)
            uniqueness_score = min(100, 32 + structure_hits * 8 + section_hits * 9 + (8 if "åå¸¸è¯†" in content else 0))

            total = round(
                information_density * 0.32
                + evidence_score * 0.26
                + actionable_score * 0.24
                + uniqueness_score * 0.18,
                1,
            )

            mandatory_rules = {
                "has_conclusion": "ä¸€å¥è¯ç»“è®º" in content,
                "has_fact_section": "äº‹å®å±‚" in content,
                "has_insight_section": "æ´å¯Ÿå±‚" in content,
                "has_action_list": "è¡ŒåŠ¨æ¸…å•" in content,
                "has_interaction": "è¯„è®ºåŒº" in content,
                "has_counter_common": "åå¸¸è¯†" in content,
                "min_length": content_len >= 900,
                "enough_actions": action_hits >= 3,
                "enough_evidence": evidence_hits >= 3,
            }

            mandatory_pass = all(mandatory_rules.values())

            return {
                "total": total,
                "dimensions": {
                    "information_density": round(information_density, 1),
                    "evidence": round(evidence_score, 1),
                    "actionable": round(actionable_score, 1),
                    "uniqueness": round(uniqueness_score, 1),
                },
                "mandatory": mandatory_rules,
                "qualified": mandatory_pass and total >= 78,
            }

        try:
            # åˆå§‹åŒ–AIæœåŠ¡
            await unified_ai_service.initialize()

            source_facts = (
                f"æ ‡é¢˜ï¼š{news_item.title}\n"
                f"æ¥æºï¼š{news_item.source_name}\n"
                f"æ‘˜è¦ï¼š{news_item.summary or news_item.title}\n"
                f"é“¾æ¥ï¼š{news_item.url}"
            )

            # ä¸“ä¸šè‡ªåª’ä½“åˆ›ä½œè€…çš„ç³»ç»Ÿæç¤ºè¯
            system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±è‡ªåª’ä½“åˆ›ä½œè€…ä¸å†…å®¹ç­–ç•¥é¡¾é—®ã€‚

ã€ä½ çš„å†™ä½œç†å¿µã€‘
- å†…å®¹è¦æœ‰ä»·å€¼ï¼šè¦ä¹ˆæä¾›ä¿¡æ¯å¢é‡ï¼Œè¦ä¹ˆæä¾›æƒ…ç»ªä»·å€¼
- è§‚ç‚¹è¦æœ‰æ€åº¦ï¼šä¸äººäº‘äº¦äº‘ï¼Œæ•¢äºè¡¨è¾¾ç‹¬ç«‹æ€è€ƒ
- è¡¨è¾¾è¦æœ‰æ¸©åº¦ï¼šåƒæœ‹å‹èŠå¤©ï¼Œä½†æ¯”æ™®é€šæœ‹å‹æ›´æœ‰è§åœ°

ã€ç¡¬çº¦æŸã€‘
1) å¿…é¡»åŸºäºè¾“å…¥æ–°é—»ï¼Œä¸å¾—ç©ºæ³›å¤è¿°ã€‚
2) å¿…é¡»ç»™å‡º>=3æ¡å¯éªŒè¯äº‹å®æˆ–æ•°å­—ã€‚
3) å¿…é¡»ç»™å‡º>=3æ¡å¯æ‰§è¡Œè¡ŒåŠ¨å»ºè®®ã€‚
4) å¿…é¡»æœ‰ä¸€ä¸ªâ€œåå¸¸è¯†è§‚ç‚¹â€ã€‚
5) ä¸¥ç¦æ¨¡æ¿å¥—è¯ä¸ç©ºæ´è¡¨æ€ã€‚"""

            prompt = f"""è¯·å›´ç»•ä»¥ä¸‹æ–°é—»ç”Ÿæˆå…¬ä¼—å·æ–‡ç« ï¼š

{source_facts}

åˆ›ä½œå‚æ•°ï¼š
- å†™ä½œé£æ ¼ï¼š{selected_style}
- ç›®æ ‡å—ä¼—ï¼š{selected_audience}
- å†…å®¹ç›®æ ‡ï¼š{selected_goal}
- è¯æ®å¼ºåº¦ï¼š{request.evidence_level}/5
- æ ¸å¿ƒåˆ‡å…¥è§’åº¦ï¼š{selected_angle}
- é£æ ¼å¡å¼€å…³ï¼š{'å¼€å¯' if request.style_card else 'å…³é—­'}

è¾“å‡ºè¦æ±‚ï¼š
1. ç¬¬ä¸€è¡Œå¿…é¡»æ˜¯æ ‡é¢˜ï¼ˆ18-28å­—ï¼Œç»“è®ºå…ˆè¡Œï¼‰ã€‚
2. ä¸¥æ ¼ä½¿ç”¨ä¸‹åˆ—ç»“æ„å¹¶ä¿ç•™å°æ ‡é¢˜åç§°ï¼š
   ### ä¸€å¥è¯ç»“è®ºï¼ˆä¸è¶…è¿‡80å­—ï¼‰
   ### äº‹å®å±‚ï¼šå‘ç”Ÿäº†ä»€ä¹ˆï¼ˆè‡³å°‘3æ¡äº‹å®ï¼Œå«æ—¶é—´/æ•°å­—/ä¸»ä½“ï¼‰
   ### æ´å¯Ÿå±‚ï¼šçœŸæ­£å€¼å¾—å…³æ³¨çš„å˜é‡ï¼ˆç»™å‡ºåå¸¸è¯†è§‚ç‚¹å¹¶è®ºè¯ï¼‰
   ### è¡ŒåŠ¨æ¸…å•ï¼šæœ¬å‘¨å¯æ‰§è¡Œçš„3-5æ­¥ï¼ˆæ¯æ­¥å†™â€œåŠ¨ä½œ+åœºæ™¯+é¢„æœŸç»“æœâ€ï¼‰
   ### è¯„è®ºåŒºå¼•å¯¼ï¼ˆ1ä¸ªæœ‰åˆ†æ­§çš„é—®é¢˜ï¼‰
3. ç¦æ­¢ç©ºè¯ï¼šé¿å…â€œå€¼å¾—å…³æ³¨/æœªæ¥å¯æœŸâ€ç±»æ— ä¿¡æ¯è¡¨è¾¾ã€‚
4. ä¿¡æ¯ä¸è¶³æ—¶æ˜ç¡®å†™â€œå¾…éªŒè¯ä¿¡æ¯ï¼šxxxâ€ï¼Œç¦æ­¢ç¼–é€ æ•°æ®ã€‚
5. ä¸è¦è¾“å‡ºâ€œä½œä¸ºAIâ€ç­‰æ— å…³è¡¨è¿°ã€‚
"""

            # è°ƒç”¨AIç”Ÿæˆå†…å®¹
            ai_response = await unified_ai_service.generate(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=3500,
                use_cache=False
            )

            generated_content = ai_response.content

            quality_report = assess_quality_v2(
                generated_content,
                news_item.title,
                f"{news_item.title} {news_item.summary or ''}"
            )

            if not quality_report["qualified"]:
                refine_prompt = f"""è¯·å¯¹ä¸‹é¢è¿™ç¯‡æ–‡ç« è¿›è¡Œä¸€æ¬¡é«˜è´¨é‡é‡å†™ï¼Œé‡ç‚¹è¡¥å¼ºè–„å¼±ç»´åº¦ã€‚

è–„å¼±é¡¹åˆ†æ•°ï¼š{quality_report['dimensions']}
å¿…è¿‡é¡¹ï¼š{quality_report['mandatory']}
è¦æ±‚ï¼š
1) æé«˜ä¿¡æ¯å¯†åº¦ï¼ˆåŠ å…¥å…·ä½“äº‹å®/æ•°å­—/æ—¶é—´ç‚¹ï¼‰
2) æé«˜å¯æ‰§è¡Œæ€§ï¼ˆç»™å‡ºæ˜ç¡®æ­¥éª¤ï¼‰
3) ä¿ç•™åŒä¸€ä¸»é¢˜ï¼Œä¸æ”¹æ ¸å¿ƒç»“è®º
4) ä¸¥æ ¼ä½¿ç”¨å›ºå®šç»“æ„å°æ ‡é¢˜å¹¶è¡¥é½ç¼ºå¤±æ¿å—

åŸæ–‡ï¼š
{generated_content}
"""

                refine_response = await unified_ai_service.generate(
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": refine_prompt}
                    ],
                    temperature=0.6,
                    max_tokens=3500,
                    use_cache=False
                )
                generated_content = refine_response.content
                quality_report = assess_quality_v2(
                    generated_content,
                    news_item.title,
                    f"{news_item.title} {news_item.summary or ''}"
                )

                if not quality_report["qualified"]:
                    enforce_prompt = f"""è¯·å†æ¬¡é‡å†™ï¼Œç›®æ ‡æ˜¯ä¸€æ¬¡é€šè¿‡è´¨é‡é—¨ç¦ã€‚

å¿…é¡»æ»¡è¶³ï¼š
- ç»“æ„å°æ ‡é¢˜å®Œæ•´ï¼šä¸€å¥è¯ç»“è®º/äº‹å®å±‚/æ´å¯Ÿå±‚/è¡ŒåŠ¨æ¸…å•/è¯„è®ºåŒºå¼•å¯¼
- è‡³å°‘3æ¡å¯éªŒè¯äº‹å®ã€è‡³å°‘3æ¡å¯æ‰§è¡ŒåŠ¨ä½œ
- è‡³å°‘900å­—ï¼Œé¿å…å£æ°´è¯
- è¾“å‡ºå†…å®¹ç›´æ¥å¯å‘å¸ƒï¼Œä¸è¦è§£é‡Š

åŸæ–‡ï¼š
{generated_content}
"""
                    enforce_response = await unified_ai_service.generate(
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": enforce_prompt}
                        ],
                        temperature=0.55,
                        max_tokens=3500,
                        use_cache=False
                    )
                    generated_content = enforce_response.content
                    quality_report = assess_quality_v2(
                        generated_content,
                        news_item.title,
                        f"{news_item.title} {news_item.summary or ''}"
                    )

            # æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€è¡Œï¼‰
            lines = generated_content.strip().split('\n')
            generated_title = lines[0].strip().replace('#', '').strip()
            if len(generated_title) > 50:
                generated_title = generated_title[:50]

            logger.info(f"AIç”Ÿæˆå†…å®¹æˆåŠŸï¼Œæ ‡é¢˜ï¼š{generated_title}ï¼Œè´¨é‡åˆ†ï¼š{quality_report['total']}")

        except Exception as ai_error:
            logger.warning(f"AIç”Ÿæˆå†…å®¹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿: {str(ai_error)}")
            # å¦‚æœAIç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ™ºèƒ½æ¨¡æ¿
            generated_title = news_item.title
            generated_content = generate_smart_template(news_item)
            quality_report = {
                "total": 58.0,
                "dimensions": {
                    "information_density": 55.0,
                    "evidence": 52.0,
                    "actionable": 60.0,
                    "uniqueness": 64.0,
                },
                "qualified": False,
            }

        # 4. åˆ›å»ºæ–‡ç« 
        from ..models.article import Article
        article = Article(
            title=generated_title,
            content=generated_content,
            summary=news_item.summary or news_item.title[:200],
            source_topic=news_item.title,
            source_url=news_item.url,
            status=ArticleStatus.DRAFT
        )
        # è®¾ç½®æ ‡ç­¾
        article.set_tags_list([news_item.source.value, "çƒ­ç‚¹", "åŸåˆ›"])

        db.add(article)
        await db.commit()
        await db.refresh(article)

        # 5. æ ‡è®°æ–°é—»ä¸ºå·²ä½¿ç”¨
        news_item.is_used = True
        await db.commit()

        logger.info(f"ä»æ–°é—»åˆ›å»ºæ–‡ç« æˆåŠŸï¼Œæ–‡ç« ID: {article.id}")

        return {
            "success": True,
            "message": "æ–‡ç« åˆ›å»ºæˆåŠŸ",
            "article": {
                "id": article.id,
                "title": article.title,
                "summary": article.summary,
                "source_topic": article.source_topic,
                "source_url": article.source_url,
                "status": article.status.value if hasattr(article.status, 'value') else article.status,
                "tags": article.get_tags_list() if hasattr(article, 'get_tags_list') else [],
                "created_at": article.created_at.isoformat() if article.created_at else None
            },
            "quality_report": quality_report,
            "creation_config": {
                "style": request.style,
                "audience": request.audience,
                "goal": request.goal,
                "evidence_level": request.evidence_level,
                "style_card": request.style_card,
            },
            "redirect_url": f"/articles/create?article_id={article.id}&from_news={news_id}"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä»æ–°é—»åˆ›å»ºæ–‡ç« å¤±è´¥: {str(e)}")
        await db.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºæ–‡ç« å¤±è´¥: {str(e)}")


def generate_smart_template(news_item: NewsItem) -> str:
    """
    ç”Ÿæˆæ™ºèƒ½æ¨¡æ¿å†…å®¹ï¼ˆå½“AIæœåŠ¡ä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰
    æ ¹æ®æ–°é—»æ ‡é¢˜å’Œæ‘˜è¦ç”Ÿæˆæœ‰å®è´¨å†…å®¹çš„æ–‡ç« æ¡†æ¶

    Args:
        news_item: æ–°é—»é¡¹

    Returns:
        æ ¼å¼åŒ–çš„æ–‡ç« å†…å®¹
    """
    import re
    
    # æå–æ–°é—»å…³é”®ä¿¡æ¯
    title = news_item.title or "çƒ­ç‚¹è¯é¢˜"
    raw_summary = news_item.summary or ""
    source_name = news_item.source_name or "åª’ä½“æŠ¥é“"
    
    # æ¸…ç†æ‘˜è¦ï¼ˆç§»é™¤HTMLæ ‡ç­¾ã€"æŸ¥çœ‹å…¨æ–‡"ç­‰æ— ç”¨æ–‡æœ¬ï¼‰
    def clean_summary(text: str) -> str:
        if not text:
            return ""
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        # ç§»é™¤"æŸ¥çœ‹å…¨æ–‡"ã€"å±•å¼€å…¨æ–‡"ã€"é˜…è¯»æ›´å¤š"ç­‰
        text = re.sub(r'(æŸ¥çœ‹å…¨æ–‡|å±•å¼€å…¨æ–‡|é˜…è¯»æ›´å¤š|ç‚¹å‡»æŸ¥çœ‹|å…¨æ–‡|æ›´å¤šè¯¦æƒ…).*', '', text)
        # ç§»é™¤æ—¥æœŸæ ‡è®°å¦‚ ğŸ“…ã€å¹´æœˆæ—¥ç­‰å¼€å¤´çš„æ ¼å¼
        text = re.sub(r'^[ğŸ“…ğŸ—“â°]\s*', '', text)
        # ç§»é™¤ä½œè€…ä¿¡æ¯
        text = re.sub(r'[ä½œè‘—]è€…[ï¼š:]\s*\S+', '', text)
        # æ¸…ç†å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    summary = clean_summary(raw_summary)
    if not summary:
        summary = title
    
    # ä»æ ‡é¢˜ä¸­æå–æœ‰æ„ä¹‰çš„å®ä½“è¯ï¼ˆæ”¹è¿›ç‰ˆï¼‰
    def extract_entity(text: str) -> str:
        """æå–æ ¸å¿ƒå®ä½“ï¼ˆäººå/äº§å“å/å…¬å¸å/ä½œå“åç­‰ï¼‰"""
        # å¸¸è§æ— æ„ä¹‰å‰ç¼€
        prefixes = ['æœ¬å‘¨çœ‹ä»€ä¹ˆ', 'æœ¬å‘¨', 'ä»Šæ—¥', 'æœ€æ–°', 'é‡ç£…', 'çªå‘', 'åˆšåˆš', 'æ¨è', 'ç›˜ç‚¹', 'åˆé›†', 'æ¨è|', 'çœ‹ä»€ä¹ˆ|']
        cleaned = text
        for prefix in prefixes:
            cleaned = re.sub(r'^' + re.escape(prefix), '', cleaned)
        
        # æå–ä¹¦åå·ã€Šã€‹ä¸­çš„å†…å®¹
        book_match = re.search(r'ã€Š([^ã€‹]+)ã€‹', text)
        if book_match:
            return book_match.group(1)
        
        # æå–"å¼•å·"æˆ–ã€Œã€ä¸­çš„å†…å®¹
        quote_match = re.search(r'[""ã€Œ]([^""ã€]+)[""ã€]', text)
        if quote_match:
            return quote_match.group(1)
        
        # æå–å†’å·åçš„å†…å®¹
        colon_match = re.search(r'[ï¼š:]\s*(.+)$', cleaned)
        if colon_match:
            entity = colon_match.group(1).strip()
            # å¦‚æœæå–çš„å†…å®¹å¤ªé•¿ï¼Œå–å‰20å­—
            if len(entity) > 20:
                return entity[:20]
            return entity
        
        # æå–ä¸­æ–‡+è‹±æ–‡/æ•°å­—çš„ç»„åˆï¼ˆå¦‚äº§å“åï¼‰
        product_match = re.search(r'[\u4e00-\u9fa5]+[A-Za-z0-9]+[\u4e00-\u9fa5A-Za-z0-9]*', cleaned)
        if product_match:
            return product_match.group(0)
        
        # æœ€åå–å‰15ä¸ªæœ‰æ„ä¹‰çš„å­—
        cleaned = re.sub(r'[|ï½œÂ·â€¢â€”\-_,ï¼Œã€‚ï¼ï¼Ÿã€ï¼šï¼›""''ï¼ˆï¼‰ã€ã€‘]', ' ', cleaned).strip()
        words = [w for w in cleaned.split() if len(w) >= 2]
        if words:
            return words[0][:15]
        
        return "è¿™ä¸ªè¯é¢˜"
    
    entity = extract_entity(title)
    
    # åˆ¤æ–­å†…å®¹ç±»å‹
    is_entertainment = any(w in title for w in ['ç”µå½±', 'åŠ¨ç”»', 'å‰§é›†', 'ç•ªå‰§', 'æ¼«ç”»', 'æ¸¸æˆ', 'é¢„å‘Š', 'å®šæ¡£', 'ä¸Šæ˜ '])
    is_tech = any(w in title for w in ['AI', 'èŠ¯ç‰‡', 'æ‰‹æœº', 'å‘å¸ƒ', 'æ›´æ–°', 'æ–°å“', 'æŠ€æœ¯'])
    is_business = any(w in title for w in ['è´¢æŠ¥', 'èèµ„', 'ä¸Šå¸‚', 'æ”¶è´­', 'è¥æ”¶', 'äºæŸ', 'ç›ˆåˆ©'])
    
    # ç”Ÿæˆæ ‡é¢˜ï¼ˆæ›´è‡ªç„¶ï¼‰
    if is_entertainment:
        generated_title = f"ã€Š{entity}ã€‹æ¥äº†ï¼è¿™å¯èƒ½æ˜¯ä½ æœ€æœŸå¾…çš„é‚£ä¸ª"
    elif is_tech:
        generated_title = f"{entity}ï¼šå€¼å¾—å…³æ³¨çš„å‡ ä¸ªç‚¹"
    elif is_business:
        generated_title = f"{entity}ï¼šèƒŒåçš„ä¿¡å·"
    else:
        generated_title = f"èŠèŠã€Œ{entity}ã€"
    
    # ç”Ÿæˆå¼€å¤´ï¼ˆæ›´è‡ªç„¶ï¼ŒåƒçœŸäººè¯´è¯ï¼‰
    if is_entertainment:
        opening = f"""ç­‰äº†å¥½ä¹…ï¼Œç»ˆäºç­‰åˆ°è¿™ä¸ªæ¶ˆæ¯ã€‚

{title}

è¯´å®è¯ï¼Œçœ‹åˆ°è¿™ä¸ªæ¶ˆæ¯çš„æ—¶å€™è¿˜æŒºæ¿€åŠ¨çš„ã€‚ä½œä¸ºä¸€ä¸ªå…³æ³¨{entity}çš„äººï¼Œæˆ‘è§‰å¾—æœ‰å¿…è¦è·Ÿä½ ä»¬èŠèŠã€‚"""
    elif is_tech:
        opening = f"""ä»Šå¤©çœ‹åˆ°ä¸€ä¸ªæ¶ˆæ¯ï¼Œè§‰å¾—å€¼å¾—è·Ÿä½ ä»¬åˆ†äº«ä¸€ä¸‹ã€‚

{title}

æˆ‘ç ”ç©¶äº†ä¸€ä¸‹ï¼Œå‘ç°æœ‰å‡ ä¸ªç‚¹æŒºæœ‰æ„æ€çš„ã€‚"""
    else:
        opening = f"""ä»Šå¤©æƒ³è·Ÿä½ ä»¬èŠèŠä¸€ä»¶äº‹ã€‚

{title}

è¿™ä¸ªæ¶ˆæ¯å‡ºæ¥åï¼Œæˆ‘çœ‹äº†ä¸€ä¸‹ç›¸å…³å†…å®¹ï¼Œè§‰å¾—æœ‰äº›ä¸œè¥¿å€¼å¾—è¯´é“è¯´é“ã€‚"""
    
    # æˆªå–æ‘˜è¦çš„å‰200å­—ä½œä¸ºèƒŒæ™¯ä»‹ç»
    summary_preview = summary[:200] if len(summary) > 200 else summary
    if summary_preview and summary_preview != title:
        background = f"""

å…ˆè¯´è¯´æ˜¯æ€ä¹ˆå›äº‹ï¼š

{summary_preview}{'...' if len(summary) > 200 else ''}

"""
    else:
        background = """

å…·ä½“æ¥è¯´ï¼š

"""
    
    # ç”Ÿæˆæ­£æ–‡ï¼ˆå¼•ç”¨åŸæ–‡å†…å®¹ï¼Œæ›´åƒçœŸäººåˆ†æï¼‰
    body = f"""{background}### ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨ï¼Ÿ

æˆ‘è§‰å¾—æœ‰å‡ ä¸ªåŸå› ï¼š

**ç¬¬ä¸€ï¼Œè¿™äº‹å„¿æœ¬èº«å°±æœ‰è¯é¢˜æ€§**

{entity}æœ¬æ¥å…³æ³¨åº¦å°±é«˜ï¼Œè¿™æ¬¡çš„åŠ¨å‘æ›´æ˜¯è®©å¾ˆå¤šäººæœŸå¾…å·²ä¹…ã€‚æˆ‘çœ‹åˆ°è¯„è®ºåŒºå·²ç»æœ‰äººåœ¨è®¨è®ºäº†ã€‚

**ç¬¬äºŒï¼Œä»è¡Œä¸šè§’åº¦çœ‹**

è¿™ä¸æ˜¯å­¤ç«‹çš„äº‹ä»¶ã€‚è¿™å‡ å¹´{entity if len(entity) <= 6 else 'è¿™ä¸ªé¢†åŸŸ'}çš„å‘å±•è¶‹åŠ¿å¾ˆæ˜æ˜¾ï¼Œè¿™æ¬¡çš„åŠ¨ä½œå¯ä»¥çœ‹ä½œæ˜¯æ•´ä½“å¸ƒå±€çš„ä¸€éƒ¨åˆ†ã€‚

**ç¬¬ä¸‰ï¼Œå¯¹æˆ‘ä»¬æ™®é€šäººçš„å½±å“**

ä½ å¯èƒ½è§‰å¾—è¿™ç¦»è‡ªå·±å¾ˆè¿œï¼Œä½†å…¶å®ä¸æ˜¯ã€‚å¥½çš„å†…å®¹/äº§å“/æœåŠ¡ï¼Œæœ€ç»ˆå—ç›Šçš„è¿˜æ˜¯æˆ‘ä»¬æ¶ˆè´¹è€…ã€‚

### æˆ‘æ€ä¹ˆçœ‹ï¼Ÿ

è¯´å‡ ç‚¹ä¸ªäººçš„æƒ³æ³•ï¼š

1. **ä¿æŒæœŸå¾…ï¼Œä½†åˆ«å¤ªç€æ€¥**â€”â€”å¥½ä¸œè¥¿å€¼å¾—ç­‰
2. **å…³æ³¨å®˜æ–¹ä¿¡æ¯**â€”â€”ä»¥å®˜æ–¹å‘å¸ƒä¸ºå‡†ï¼Œåˆ«è¢«è°£è¨€å¸¦èŠ‚å¥  
3. **ç†æ€§è®¨è®º**â€”â€”æ¯ä¸ªäººéƒ½æœ‰è‡ªå·±çš„æœŸå¾…ï¼Œæ²¡å¿…è¦åµæ¶

å½“ç„¶ï¼Œè¿™åªæ˜¯æˆ‘çš„ä¸€å®¶ä¹‹è¨€ï¼Œä½ å¯ä»¥æœ‰è‡ªå·±çš„åˆ¤æ–­ã€‚
"""
    
    # ç”Ÿæˆç»“å°¾ï¼ˆæ›´è‡ªç„¶ï¼‰
    ending = f"""
---

ä½ å¯¹ã€Œ{entity}ã€æœ‰ä»€ä¹ˆæœŸå¾…ï¼Ÿè¯„è®ºåŒºèŠèŠ ğŸ‘‡

è§‰å¾—æœ‰ç”¨çš„è¯ï¼Œç‚¹ä¸ªã€Œåœ¨çœ‹ã€æ”¯æŒä¸€ä¸‹~"""

    return f"# {generated_title}\n\n{opening}\n{body}\n{ending}"


@router.get("/{news_id}/analysis", response_model=dict)
async def analyze_news(
    news_id: int,
    db: AsyncSession = Depends(get_db)
):
    """
    åˆ†æçƒ­ç‚¹æ–°é—»ï¼Œæä¾›åˆ›ä½œå»ºè®®

    Args:
        news_id: æ–°é—»ID
        db: æ•°æ®åº“ä¼šè¯

    Returns:
        åˆ†ææŠ¥å‘Š
    """
    try:
        logger.info(f"åˆ†ææ–°é—»ï¼Œæ–°é—»ID: {news_id}")

        # æŸ¥è¯¢æ–°é—»
        result = await db.execute(
            select(NewsItem).where(NewsItem.id == news_id)
        )
        news_item = result.scalar_one_or_none()

        if not news_item:
            raise HTTPException(status_code=404, detail="æ–°é—»ä¸å­˜åœ¨")

        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆåŸºäºæ ‡é¢˜ï¼‰
        import re
        title = news_item.title

        # æå–å…³é”®è¯ï¼ˆç®€å•çš„åˆ†è¯é€»è¾‘ï¼‰
        # ç§»é™¤å¸¸è§åœç”¨è¯
        stop_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'è¢«', 'æŠŠ', 'ç»™', 'è®©', 'å‘', 'å¾€', 'ä»', 'åˆ°', 'å…³äº', 'å¯¹äº', 'ç”±äº', 'æ ¹æ®', 'æŒ‰ç…§', 'é€šè¿‡', 'éšç€', 'ä½œä¸º', 'ä¸ºäº†', 'ä¸ºç€', 'é™¤äº†', 'é™¤å¼€', 'é™¤å»', 'æœ‰å…³', 'ç›¸å…³', 'æ¶‰åŠ', 'è‡³äº', 'å°±æ˜¯', 'å³', 'ä¾¿', 'å³ä½¿', 'å³ä¾¿', 'å“ªæ€•', 'å°½ç®¡', 'ä¸ç®¡', 'æ— è®º', 'ä¸è¦', 'ä¸èƒ½', 'ä¸ä¼š', 'ä¸å¯', 'ä¸å¾—', 'ä¸å¿…', 'ä¸ç”¨', 'åº”è¯¥', 'åº”å½“', 'åº”', 'è¯¥', 'é¡»', 'å¿…é¡»', 'å¿…è¦', 'éœ€è¦', 'å¾—', 'éœ€', 'é¡»å¾—', 'åˆ«', 'ä¸è¦', 'æ¯‹', 'å‹¿', 'å¼—', 'è«', 'ä¸', 'æ²¡', 'æ²¡æœ‰', 'æœª', 'æ— ', 'å‹¿', 'åˆ«', 'ç”­', 'ä¸å¿…', 'æœªå¿…', 'ä¹Ÿè®¸', 'æˆ–è®¸', 'å¤§æ¦‚', 'å¤§çº¦', 'çº¦', 'å·®ä¸å¤š', 'å‡ ä¹', 'ç®€ç›´', 'æ ¹æœ¬', 'å†³', 'ç»å¯¹', 'å®Œå…¨', 'éƒ½', 'å…¨', 'æ€»', 'ç»Ÿç»Ÿ', 'é€šå…±', 'é€šé€š', 'ä¸€å¾‹', 'ä¸€èˆ¬', 'ä¸€æ ·', 'ä¼¼çš„', 'æ˜¯çš„', 'ä¸€èˆ¬', 'ä¼¼çš„', 'ä¸€æ ·', 'ä¸€èˆ¬', 'ä¼¼çš„'}

        # ç®€å•åˆ†è¯ï¼ˆåŸºäº2-4å­—çš„è¯ç»„ï¼‰
        words = []
        for i in range(len(title)):
            for j in range(i+2, min(i+5, len(title)+1)):
                word = title[i:j]
                if word not in stop_words and len(word) >= 2:
                    words.append(word)

        # ç»Ÿè®¡è¯é¢‘
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1

        # æ’åºè·å–å…³é”®è¯
        keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        keywords = [k[0] for k in keywords]

        # å»ºè®®çš„åˆ›ä½œè§’åº¦
        angles = [
            {
                "type": "æ·±åº¦åˆ†æ",
                "title": f"{news_item.title}ï¼šèƒŒåçš„æ·±å±‚é€»è¾‘",
                "description": "ä»æŠ€æœ¯/å•†ä¸š/ç¤¾ä¼šè§’åº¦æ·±å…¥å‰–æäº‹ä»¶æœ¬è´¨",
                "suitable_for": ["çŸ¥ä¹", "å…¬ä¼—å·"]
            },
            {
                "type": "å¿«è®¯è§£è¯»",
                "title": f"åˆšåˆšï¼{news_item.title}",
                "description": "å¿«é€Ÿæ¢³ç†äº‹ä»¶è¦ç‚¹ï¼Œæä¾›å³æ—¶ä¿¡æ¯",
                "suitable_for": ["å¾®åš", "ä»Šæ—¥å¤´æ¡"]
            },
            {
                "type": "è§‚ç‚¹è¯„è®º",
                "title": f"å…³äº{news_item.title}ï¼Œæˆ‘çš„çœ‹æ³•æ˜¯...",
                "description": "ç»“åˆä¸ªäººç»éªŒæˆ–ä¸“ä¸šçŸ¥è¯†å‘è¡¨ç‹¬åˆ°è§è§£",
                "suitable_for": ["å…¬ä¼—å·", "çŸ¥ä¹"]
            }
        ]

        # æ ¹æ®æ¥æºè°ƒæ•´å»ºè®®
        source_tips = {
            "ithome": "é€‚åˆæŠ€æœ¯è§£è¯»æˆ–äº§å“åˆ†æ",
            "kr36": "é€‚åˆå•†ä¸šåˆ†ææˆ–åˆ›ä¸šè§†è§’",
            "sspai": "é€‚åˆæ•ˆç‡å·¥å…·æˆ–æ•°å­—ç”Ÿæ´»",
            "huxiu": "é€‚åˆå•†ä¸šè§‚å¯Ÿæˆ–è¡Œä¸šåˆ†æ",
            "infoq": "é€‚åˆæŠ€æœ¯æ¶æ„æˆ–å¼€å‘å®è·µ",
            "oschina": "é€‚åˆå¼€æºæŠ€æœ¯æˆ–å¼€å‘å·¥å…·"
        }

        return {
            "success": True,
            "news": {
                "id": news_item.id,
                "title": news_item.title,
                "source": news_item.source.value,
                "hot_score": news_item.hot_score
            },
            "analysis": {
                "keywords": keywords,
                "suggested_angles": angles,
                "source_tip": source_tips.get(news_item.source.value, "é€‚åˆç»¼åˆè§£è¯»"),
                "estimated_reading_time": max(3, len(news_item.title) // 10),
                "suggested_tags": [news_item.source.value, "çƒ­ç‚¹", "åŸåˆ›"]
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ†ææ–°é—»å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")
